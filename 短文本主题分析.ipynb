{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD,NMF,LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_clearcut_topics():\n",
    "    ## for demostration purpose, don't take it personally : )\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [1000, 1000])\n",
    "\n",
    "def generate_unbalanced_topics():\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [10, 1000])\n",
    "\n",
    "def generate_semantic_context_topics():\n",
    "    return np.repeat([\"we love bergers\"\n",
    "                      , \"we hate bergers\"\n",
    "                      , \"we love sandwiches\"\n",
    "                      , \"we hate sandwiches\"], 1000)\n",
    "\n",
    "def generate_noisy_topics():\n",
    "    def _random_typos(word, n):\n",
    "        typo_index = np.random.randint(0, len(word), n)\n",
    "        return [word[:i]+\"X\"+word[i+1:] for i in typo_index]\n",
    "    t1 = [\"we love %s\" % w for w in _random_typos(\"bergers\", 15)]\n",
    "    t2 = [\"we hate %s\" % w for w in _random_typos(\"sandwiches\", 15)]\n",
    "    return np.r_[t1, t2]\n",
    "\n",
    "sample_texts = {\n",
    "     \"clearcut topics\": generate_clearcut_topics()\n",
    "    , \"unbalanced topics\": generate_unbalanced_topics()\n",
    "    , \"semantic topics\": generate_semantic_context_topics()\n",
    "    , \"noisy topics\": generate_noisy_topics()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noisy topics\n",
      "[('we hate saXdwiches', 5), ('we love beXgers', 5), ('we love bergerX', 3), ('we love berXers', 3), ('we hate Xandwiches', 3), ('we love bergXrs', 2), ('we hate sXndwiches', 2), ('we love Xergers', 1), ('we hate sandwiXhes', 1), ('we hate sandwicheX', 1), ('we hate sandwichXs', 1), ('we hate sandwicXes', 1), ('we hate sandXiches', 1), ('we love bergeXs', 1)]\n",
      "\n",
      "clearcut topics\n",
      "[('we love bergers', 1000), ('we hate sandwiches', 1000)]\n",
      "\n",
      "unbalanced topics\n",
      "[('we hate sandwiches', 1000), ('we love bergers', 10)]\n",
      "\n",
      "semantic topics\n",
      "[('we love bergers', 1000), ('we love sandwiches', 1000), ('we hate sandwiches', 1000), ('we hate bergers', 1000)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for desc,texts in sample_texts.items():\n",
    "    print desc\n",
    "    print Counter(texts).most_common()\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_topic(texts,topic_model,n_topics,vec_model='tf',thr=1e-2,**kwargs):\n",
    "    vectorizer=CountVectorizer() if vec_model=='tf' else TfidfVectorizer()\n",
    "    text_vec=vectorizer.fit_transform(texts)\n",
    "    words=np.array(vectorizer.get_feature_names())\n",
    "    topic_models={'nmf':NMF,'svd':TruncatedSVD,'lda':LatentDirichletAllocation,'kmeans':KMeans}\n",
    "    topicfinder=topic_models[topic_model](n_topics,**kwargs).fit(text_vec)\n",
    "    topic_dists=topicfinder.components_ if topic_model is not \"kmeans\" else topicfinder.cluster_centers_\n",
    "    topic_dists/=topic_dists.max(axis=1).reshape((-1,1))\n",
    "    def _topic_keywords(topic_dist):\n",
    "        keywords_index=np.abs(topic_dist)>=thr\n",
    "        keywords_prefix=np.where(np.sign(topic_dist)>0,\"\",\"^\")[keywords_index]\n",
    "        keywords=' | '.join(map(lambda x:''.join(x),zip(keywords_prefix,words[keywords_index])))\n",
    "        return keywords\n",
    "    topic_keywords=map(_topic_keywords,topic_dists)\n",
    "    return \"\\n\".join(\"Topic %i: %s\" % (i, t) for i, t in enumerate(topic_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches\n",
      "Topic 2: bergers | hate | love | sandwiches | ^we\n",
      "Topic 3: ^bergers | ^hate | love | sandwiches\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts['clearcut topics'],'svd',4,vec_model='tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches\n",
      "Topic 2: ^bergers | ^hate | ^love | ^sandwiches | we\n",
      "Topic 3: bergers | ^hate | ^love | sandwiches\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts['clearcut topics'],'svd',4,vec_model='tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches | we\n",
      "Topic 2: bergers | hate | love | sandwiches | ^we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts['unbalanced topics'],'svd',3,vec_model='tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergerx | bergexs | bergxrs | berxers | bexgers | hate | love | sandwichex | sandwichxs | sandwicxes | sandwixhes | sandxiches | saxdwiches | sxndwiches | we | xandwiches | xergers\n",
      "Topic 1: ^bergerx | ^bergexs | ^bergxrs | ^berxers | ^bexgers | hate | ^love | sandwichex | sandwichxs | sandwicxes | sandwixhes | sandxiches | saxdwiches | sxndwiches | we | xandwiches | ^xergers\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts['noisy topics'],'svd',2,vec_model='tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hate | sandwiches | we\n",
      "Topic 1: bergers | love | we\n",
      "Topic 2: hate | sandwiches | we\n",
      "Topic 3: bergers | love | we\n",
      "Topic 4: bergers | love | we\n",
      "Topic 5: bergers | love | we\n",
      "Topic 6: bergers | love | we\n",
      "Topic 7: bergers | love | we\n",
      "Topic 8: bergers | love | we\n",
      "Topic 9: bergers | love | we\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts['clearcut topics'],'kmeans',10,vec_model='tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
